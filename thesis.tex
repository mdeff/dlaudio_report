\documentclass[a4paper,12pt,twoside]{report}

\usepackage[T1]{fontenc} \usepackage{lmodern} \usepackage[utf8]{inputenc}
\usepackage[english]{babel} \usepackage{csquotes}
\usepackage{float} \usepackage{graphicx,subcaption}
\usepackage{amssymb,amsmath} \usepackage{siunitx}
\usepackage[style=numeric,backend=biber]{biblatex} \bibliography{refs}
\usepackage[top=4cm,bottom=4cm,outer=3cm,inner=4cm]{geometry}
\usepackage{fancyhdr} \pagestyle{fancy} %\usepackage{lastpage}
%\usepackage{parskip} \setlength{\parindent}{0em} \setlength{\parskip}{1em}
\usepackage[colorlinks=true,allcolors=blue]{hyperref} \hypersetup{
	pdfauthor={Michaël Defferrard},
	pdftitle={Structuring Auto-Encoder},
	pdfsubject={Master Thesis in Information Technologies}
}

%\lhead{Audio Classification with Structured Deep Learning} \chead{} \rhead{}
%\cfoot{-- \thepage --}

\usepackage[acronym]{glossaries} \makeglossaries
\newacronym{EPFL}{EPFL}{École Polytechnique Fédérale de Lausanne}
\newacronym{ICASSP}{ICASSP}{International Conference on Acoustics, Speech and Signal Processing}
\newacronym{CS}{CS}{Compressed Sensing}
\newacronym{ANNs}{ANNs}{Artificial Neural Networks}
\newacronym{CNN}{CNN}{Convolutional Neural Network}
\newacronym{RNN}{RNN}{Recursive Neural Network}
\newacronym{LSTM}{LSTM}{Long Short Term Memory}
\newacronym{MLP}{MLP}{Multi-Layer Perceptron}
\newacronym{RBM}{RBM}{Restricted Boltzmann Machine}
\newacronym{DBN}{DBN}{Deep Belief Network}
\newacronym{MIR}{MIR}{Music Information Retrieval}
\newacronym{MGR}{MGR}{Music Genre Recognition}
\newacronym{PSD}{PSD}{Predictive Sparse Decomposition}
\newacronym{CQT}{CQT}{Constant-Q Transform}
\newacronym{LCN}{LCN}{Local Contrast Normalization}
\newacronym{SVM}{SVM}{Support Vector Machine}
\newacronym{RIP}{RIP}{Restricted Isometry Property}
\newacronym{BP}{BP}{Basis Pursuit}
\newacronym{FISTA}{FISTA}{Fast Iterative Shrinkage-Thresholding Algorithm}
\newacronym{ADMM}{ADMM}{Alternating Direction Method of Multipliers}
\newacronym{PD}{PD}{Primal-Dual}
\newacronym{SGD}{SGD}{Stochastic Gradient Descent}
\newacronym{GPU}{GPU}{Graphical Processing Unit}
\newacronym{LASSO}{LASSO}{Least Absolute Shrinkage and Selection Operator}
\newacronym{AGC}{AGC}{Automatic Gain Control}
\newacronym{kNN}{kNN}{k Nearest Neighbors}
\newacronym{PCA}{PCA}{Principal Component Analysis}
\newacronym{NLDR}{NLDR}{Non-Linear Dimensionality Reduction}
\newacronym{SOM}{SOM}{Self-Organizing Map}
\newacronym{LLE}{LLE}{Locally-Linear Embedding}

\newcommand{\partref}[1]{Part~\ref{part:#1}}
\newcommand{\chapref}[1]{Chapter~\ref{chap:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\eqnref}[1]{(\ref{#1})}  % Eqn~(\ref{#1})
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argminop}{arg\,min}
\DeclareMathOperator*{\minimizeop}{minimize}
\DeclareMathOperator*{\sgn}{sgn}
\DeclareMathOperator*{\tr}{tr}
\newcommand{\argmin}[1]{\argminop\limits_{#1}}
\newcommand{\minimize}[1]{\minimizeop\limits_{#1}}  % or \min
\newcommand{\normT}[1]{\| #1 \|_2^2}
\newcommand{\normO}[1]{\| #1 \|_1}
\newcommand{\normZ}[1]{\| #1 \|_0}
\newcommand{\normF}[1]{\| #1 \|_\text{F}^2}
\newcommand{\inner}[2]{\langle #1 , #2 \rangle}
\renewcommand{\L}{\mathbf{L}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Z}{\mathbf{Z}}
\renewcommand{\d}{\mathbf{d}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\Eone}{E_1(\Z, \D)}
\newcommand{\Etwo}{E_2(\Z, \D, \E)}
\newcommand{\Ethree}{E_3(\Z, \D, \E)}
\newcommand{\G}{\mathcal{G}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\set}[2]{\{#1_i\}_{i=1}^#2}
\newcommand{\st}{\text{ s.t. }}
\newcommand{\cst}[2]{\|#1_#2\|_2 \leq 1}
%\newcommand{\forallx}[2]{\forall #1 \in \{1,\ldots,#2\}}
\newcommand{\forallx}[2]{#1 = 1, \ldots, #2}
%\newcommand{\forallx}[2]{\|#1_#2\|_2 \leq 1 \text{ for } #2 = 1,\ldots,#3}
\newcommand{\cstd}{\cst{\d}{i}, \forallx{i}{m}}
\newcommand{\cste}{\cst{\e}{k}, \forallx{k}{n}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

\newcommand*{\blankpage}{
	\thispagestyle{empty}
	\vspace*{\fill}
	\begin{center}
		\textit{This page is intentionally left blank.}
	\end{center}
	\vspace{\fill}
}
\makeatletter
\renewcommand*{\cleardoublepage}{\clearpage\if@twoside \ifodd\c@page\else
	\blankpage
	\newpage
	\if@twocolumn\hbox{}\newpage\fi\fi\fi
}
\makeatother


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%\hypersetup{pageanchor=false}
\begin{titlepage}
	
	\includegraphics[height=2.5cm]{img/logo_epfl}
	\hfill
	\includegraphics[height=2.5cm]{img/logo_lts2}
	\vspace{1.2cm}
	
	\begin{center}
		
		\textsc{\LARGE MASTER THESIS}\\
		\vspace{0.5cm}
		\large Electrical and Electronic Section\\
		\large Major in Information Technologies\\
		\vspace{0.8cm}
		
		\HRule
		\vspace{0.7cm}
		\textsc{\Large Introducing the}\\
		\vspace{0.7cm}
		\textsc{\Huge Structuring Auto-Encoder}\\
		\vspace{0.5cm}
		\textsc{\Large with an application to}\\
		\vspace{0.2cm}
		\textsc{\Large Music Genre Recognition}\\
		\vspace{0.4cm}
		\HRule
		\vspace{1.3cm}
		
		\begin{minipage}{0.45\textwidth}
			\begin{flushleft} \large
				\textbf{Student}\\ \vspace{0.5ex}
				Michaël \textsc{Defferrard} \\ \vspace{2.5ex}
				\textbf{Professor} \\ \vspace{0.5ex}
				Pierre \textsc{Vandergheynst}
			\end{flushleft}
		\end{minipage}
		\begin{minipage}{0.45\textwidth}
			\begin{flushright} \large
				\textbf{Supervisors} \\ \vspace{0.5ex}
				Xavier \textsc{Bresson} \\ \vspace{0.2ex}
				Johan \textsc{Paratte}
			\end{flushright}
		\end{minipage}
		
	\end{center}
	
	\vspace{1.0cm}
	{\large Conducted at the EPFL LTS2 laboratory.}
	\vspace{1.0cm}

	\begin{center}
		{\large June 19, 2015}
	\end{center}
	
\end{titlepage}
%\hypersetup{pageanchor=true}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\cleardoublepage
\begin{abstract}
	In this work, we present a technique to learn discriminative audio features for \gls{MIR} in an unsupervised and hierarchical way. We introduce a novel deep architecture composed by (stacks of) structured sparse auto-encoders which leverage on the recent advances of deep learning using efficient auto-encoder strategy that simultaneously learned sparse representation of inputs and dictionary adapted to those inputs. Our main goal is to find a solution to sparse coding that is structured by the proximities of the sparse features encoded by a graph of audio inputs. We borrow ideas from manifold learning to constrain our solution to be smooth on this graph, i.e. to have a small Dirichlet energy, in order to implicitly force feature proximities. Our auto-encoder model is formulated as a non-convex optimization problem, for which a {\color{red}well-posed} iterative scheme is provided. We show that the learned features combined with a simple linear classifier achieve {\color{red}110\%} accuracy in predicting genres on GTZAN.
\end{abstract}

\cleardoublepage
\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
	Xavier for day-to-day supervision. Many inputs, intuitions and references. Discussion of the results and proof reading of the thesis.
	
	Johan for intuitions at the start of the project, advices and proof reading of the thesis.
	
	Pierre
	%C’est grâce à vous que je suis satisfait de conclure ce travail éternellement inachevé par cette petite note personnelle dans une langue que je chéris beaucoup.
	
\end{abstract}

%\clearpage
%\setcounter{page}{2}
\cleardoublepage
\tableofcontents

\printglossaries
%\addcontentsline{toc}{chapter}{Glossary}
\addcontentsline{toc}{chapter}{Acronyms}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

{\color{red} Introduction to the matter.}
%\chapter{Machine learning}
% What's the purpose of learning.
% Why learned features instead of hand-crafted features ?
%\section{Supervised vs unsupervised}
% Explain the difference. Why do we want unsupervised ?
%\section{Feature / representation learning}
% NN is well suited for representation learning
% We want unsupervised feature learning vs supervised vs hand-crafted

% What will be presented. Design goal.
This thesis introduces a new auto-encoder variant which we call a \textit{structuring auto-encoder}; because it keeps the structure of the data while transforming it in another representation. As we shall see, it has the properties of an hybrid between a sparse auto-encoder \cite{lecun2006sparseAutoencoders, ranzato2007stackedSparseAutoencoders} and a denoising auto-encoder \cite{bengio2008denoisingAutoencoders}. The primary goal of the proposed algorithm is unsupervised representation learning toward the goal of feature extractions, while being robust to noisy data and fast at feature extraction (after the training phase). The quality of the extracted features shall be evaluated through a classification task.

% Plan.
This thesis is divided in two parts. Through \partref{algorithm}, the proposed model is introduced by a motivation in \chapref{motivation} and constructed step by step in \chapref{model}. \chapref{learning} describes how it learns while \chapref{related_work} compares it to existing models. As a testbed of our algorithm's performance, an application to \gls{MGR} is proposed in \partref{application} with a presentation of the problem in \chapref{problem}, our proposed system in \chapref{system} along with some implementation details in \chapref{implementation}. \chapref{results} is dedicated to the presentation of the experimental results.

This work was accomplished as a Master thesis, which is an integral part of the major in Information Technologies curriculum from the Electrical and Electronic Section of the \gls{EPFL}. It was conducted at the LTS2 laboratory\footnote{The laboratory homepage is accessible at \url{http://lts2www.epfl.ch/}.}.

While the project was ongoing, I continuously published my thoughts, observations, findings, experiments, results and plans as well as a summary of the weekly meetings with my supervisors on an online blog\footnote{My research blog is available at \url{https://lts2research.epfl.ch/blog/mdeff/}.} in the format of an open laboratory notebook. In the spirit of open research, the code as well as all the results, this report and the ongoing paper are versioned with git and available online through GitHub\footnote{My GitHub account can be found at \url{https://github.com/mdeff}.}. Some continuation of this work shall be submitted to the 41st IEEE \gls{ICASSP}. We may also aim at a journal paper at the end of the project.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\input{algorithm}
\input{application}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
% Discussion of our results. Strengths and weaknesses of our method.

\section*{Future work}
\addcontentsline{toc}{section}{Future work}

\paragraph{Encoder}
% Verify the hypothesis

\paragraph{TV norm}
% TV norm on the graph instead of Dirichlet energy

\paragraph{Distributed implementation}
% Details ?
% Train chunks of D / Z via independant threads.
% Directly in the PyUNLocBoX ?
% Usefull only if we are not memory bandwidth limited.
% Why ATLAS does not do it by default ? OpenBLAS does it.
% Move to GPU.

\paragraph{Multiple layers}
% Multiple layers of auto-encoders
% In spite of deep learning methods
% Reference to DBN: stacks of RBM, Stacked auto-encoders.

%\subsection{Deep learning}
% Rebranded as deep learning, achieve now astonishing results in a variety of fields (computer vision, NLP, machine translation, knowledge representation, automatic control)
% Deep learning: hierarchical feature learning --> we are not really using that (yet)
% Deep architectures can approximate any function (RNN any program, i.e. Turing complete)
% First fall of NN (success of kernel methods / SVM) (first AI winter ?) because perceptron was proved to not be sufficient, multi-layer was too hard (computation and training)
% Computationally expensive to train
% Needed to invent backpropagation to train multiple layers
% Hard to train: problem of vanishing gradient
% Objective function is highly non-convex.
% popularized by CNN.

%\paragraph{End-to-end learning}
% For the application, not the algorithm.
% Learn features right from raw audio. If spectrograms are good features, we should retrieve them.

%\paragraph{Domain knowledge}
% For the application, not the algorithm.
% Incorporate some domain knowledge about the dictionary via priors. Then it's not supervised anymore.
% Priors on the dictionary who are not necessarily domain specific, i.e. are general enough to apply to other kind of data.

\printbibliography
\addcontentsline{toc}{chapter}{References}

%\chapter*{Annexes}
%\addcontentsline{toc}{chapter}{Annexes}

%\section*{Conference paper}
%\addcontentsline{toc}{section}{Conference paper}
% Draft of ISMIR / ICASSP paper

%\section*{Code}
%\addcontentsline{toc}{section}{Code}
% IPython notebooks exported as LaTex.
% Available on GitHub.

\end{document}